{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fb45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import pickle as pk\n",
    "import string as st\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99241583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/anusharajan/Mtweets_republic_process.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1711a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the unnecessary columns in dataset\n",
    "df = df.drop(['Date', 'Link', 'Unnamed: 4'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b15ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing - Convert to lower case\n",
    "\n",
    "df['Tweet'] = [token.lower() for token in df['Tweet']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6f0525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Text Preprocessing - Remove hyperlinks\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing - Remove punctuations\n",
    "# Remove all punctuations from the text\n",
    "import string as st\n",
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))\n",
    "df['removed_punc'] = df['Tweet'].apply(lambda x: remove_punct(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenize'] = df['removed_punc'].apply(lambda x:word_tokenize(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162ef467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords \n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e11b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tokens'] = df['tokenize'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f81cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = df[df['Tweet'].str.contains('tablighi', na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63f8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41184aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant.to_csv('name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13459095",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.read_csv('/Users/anusharajan/Mtweets_scroll.csv')\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing - Convert to lower case\n",
    "\n",
    "sdf['Tweet'] = [token.lower() for token in sdf['Tweet']]\n",
    "sdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac08fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing - Remove hyperlinks\n",
    "sdf['Tweet'] = sdf['Tweet'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f865b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_scroll = sdf[sdf['Tweet'].str.contains('tablighi', na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4819f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_scroll.to_csv('relevant_scroll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa70bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "df = pd.read_csv('/Users/anusharajan/Mtweets_ANI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2f5abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column names\n",
    "df.columns = ['Tweet', 'Date', 'Link', 'Publication', 'Hashtag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa53550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make tweet column lowercase\n",
    "df['Tweet'] = [token.lower() for token in df['Tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f4edad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hyperlinks\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bbf1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse the tweets column for tablighi\n",
    "relevant = df[df['Tweet'].str.contains('tablighi', na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afceacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the tweets\n",
    "relevant['tokenize'] = relevant['Tweet'].apply(lambda x:word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a118704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "relevant['clean_tokens'] = relevant['tokenize'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d05fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "relevant.to_csv('relevant_ANI.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
